{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["HeC3qdGRL-5M"],"authorship_tag":"ABX9TyO50RNHGmatvnZ6KULgy+5W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multiple Input and Output Channels (7.4)\n","\n","Assume an input of shape ci × h × w and a convolution kernel of shape co × ci × kh × kw,\n","padding of (ph, pw), and stride of (sh, sw).\n","1. What is the computational cost (multiplications and additions) for the forward propagation?\n","2. What is the memory footprint?\n","3. What is the memory footprint for the backward computation?\n","4. What is the computational cost for the backpropagation?"],"metadata":{"id":"o5dDNu-FCtKm"}},{"cell_type":"code","source":["#!pip install d2l==1.0.3\n","\n","# d2l importing\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import sys\n","sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n","\n","# libraries needed\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","import d2l\n","from d2l import torch as d2l\n","\n","# Parameters\n","ci, h, w = 3, 32, 32  # Example input shape\n","co, kh, kw = 64, 3, 3  # Example convolution kernel shape\n","ph, pw = 1, 1  # Example padding\n","sh, sw = 1, 1  # Example stride\n","\n","byteSize = 4 # for Float 32 memory"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lumcSDQ7Hi_R","executionInfo":{"status":"ok","timestamp":1710971325961,"user_tz":240,"elapsed":54499,"user":{"displayName":"Jack Kalicak","userId":"01704635911466861736"}},"outputId":"861e77bd-b08b-4f39-a573-8a0025bcb8bc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["#1 Forward Computational Cost"],"metadata":{"id":"YGaCu4ChHLjL"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qh6sheBqBe7E","executionInfo":{"status":"ok","timestamp":1710971325962,"user_tz":240,"elapsed":10,"user":{"displayName":"Jack Kalicak","userId":"01704635911466861736"}},"outputId":"ec629e09-c0ce-49ac-d3b4-35825d9a73e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Forward propagation computational cost (multiplications): 1769472\n","Forward propagation computational cost (additions): 14155776\n","Forward propagation computational cost (total operations): 15925248\n"]}],"source":["# Forward propagation computational cost (multiplications and additions)\n","# accounting for stride and padding\n","h_out = (h + 2 * ph - kh) // sh + 1\n","w_out = (w + 2 * pw - kw) // sw + 1\n","\n","forward_mul = co * ci * kh * kw * h * w\n","forward_add = forward_mul * (kh * kw - 1)  # Assuming one addition per multiplication\n","\n","\n","print(\"Forward propagation computational cost (multiplications):\", forward_mul)\n","print(\"Forward propagation computational cost (additions):\", forward_add)\n","print(\"Forward propagation computational cost (total operations):\", forward_add+forward_mul)"]},{"cell_type":"markdown","source":["# 2  Memory Footprint\n","\n"],"metadata":{"id":"OFTm3cg_CyvV"}},{"cell_type":"code","source":["# Memory footprint for forward propagation\n","forward_memory = (co * h * w + ci * kh * kw) * byteSize  # Assuming 4 bytes for float32\n","print(\"Memory footprint for forward propagation:\", forward_memory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"am6JNnixH2Av","executionInfo":{"status":"ok","timestamp":1710971325962,"user_tz":240,"elapsed":6,"user":{"displayName":"Jack Kalicak","userId":"01704635911466861736"}},"outputId":"af907bbe-d349-40fe-b051-f7946f67ea4e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Memory footprint for forward propagation: 262252\n"]}]},{"cell_type":"markdown","source":["#3 Backward Computation Memory"],"metadata":{"id":"QZhlCSltHQBD"}},{"cell_type":"code","source":["backward_memory = (co * h_out * w_out + ci * kh * kw) * byteSize * 2  # Assuming gradients for weights and inputs\n","print(\"Memory footprint for backward computation:\", backward_memory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fEy71B7QHPaR","executionInfo":{"status":"ok","timestamp":1710971325963,"user_tz":240,"elapsed":6,"user":{"displayName":"Jack Kalicak","userId":"01704635911466861736"}},"outputId":"7570923f-21cc-4da7-82eb-917d46a25932"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Memory footprint for backward computation: 524504\n"]}]},{"cell_type":"markdown","source":["# 4 Backward Computational Cost\n","\n"],"metadata":{"id":"weZ1ZGEwHdIa"}},{"cell_type":"code","source":["# Backpropagation computational cost (multiplications and additions)\n","backward_mul = forward_mul * 2  # Gradients w.r.t. inputs and weights\n","backward_add = forward_add * 2  # Gradients w.r.t. inputs and weights\n","\n","print(\"Backpropagation computational cost (multiplications):\", backward_mul)\n","print(\"Backpropagation computational cost (additions):\", backward_add)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fePsF3r6MQDf","executionInfo":{"status":"ok","timestamp":1710971326245,"user_tz":240,"elapsed":286,"user":{"displayName":"Jack Kalicak","userId":"01704635911466861736"}},"outputId":"5715d4de-56d4-439f-fcac-d4a62261dae8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Backpropagation computational cost (multiplications): 3538944\n","Backpropagation computational cost (additions): 28311552\n"]}]},{"cell_type":"markdown","source":["# Other way test"],"metadata":{"id":"HeC3qdGRL-5M"}},{"cell_type":"code","source":["# # Parameters\n","# ci, h, w = 3, 32, 32  # Example input shape\n","# co, kh, kw = 64, 3, 3  # Example convolution kernel shape\n","# ph, pw = 1, 1  # Example padding\n","# sh, sw = 1, 1  # Example stride\n","# byteSize = 4 # for Float 32 memory\n","\n","# # Input tensor\n","# x = torch.randn(1, ci, h, w)\n","\n","# # Convolutional layer\n","# conv = torch.nn.Conv2d(ci, co, kernel_size=(kh, kw), padding=(ph, pw), stride=(sh, sw))\n","\n","# # Forward pass\n","# out = conv(x)\n","\n","# # Backward pass\n","# loss = torch.randn_like(out).sum()  # Example loss\n","# loss.backward()\n","\n","# # Memory footprint calculation\n","# memory_footprint = sum(p.numel() * byteSize for p in conv.parameters())  # Parameters memory\n","# memory_footprint += sum(t.numel() * byteSize for t in [x, out, loss.grad_fn.next_functions[0][0].variable] if t is not None)  # Activations and gradients memory\n","\n","# # Number of additions and multiplications calculation\n","# flops_forward = (2 * ci * kh * kw * co * (h * w) - ci * co * (h * w))  # Forward pass FLOPs\n","# flops_backward = (2 * ci * kh * kw * co * (h * w) - ci * co * (h * w)) * 2  # Backward pass FLOPs\n","\n","# print(\"Memory footprint (bytes):\", memory_footprint)\n","# print(\"Number of additions (FLOPs) for forward pass:\", flops_forward)\n","# print(\"Number of additions (FLOPs) for backward pass:\", flops_backward)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"60hfk46HL8nF","executionInfo":{"status":"error","timestamp":1710971326383,"user_tz":240,"elapsed":140,"user":{"displayName":"Jack Kalicak","userId":"01704635911466861736"}},"outputId":"094f10f3-5cb8-45b9-be48-9770949df4e4"},"execution_count":6,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-d02caa6c71e7>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Memory footprint calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"]}]}]}