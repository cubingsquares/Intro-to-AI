{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Overview - HW3\n","\n","####**Code Created by:** Matthew Ritterbusch and Jack Kalicak\n","\n","####**Due:** 17 February 2024\n","\n","\n","The code was implementing utilizing Dr. Zabaras's lecture slides he posted to his website and the D2L book. Four problems are solved within the set, each of which with their own solution file. These files can be run in Google Colabs.\n","\n","If d2l is not locally complied you must install it first utilizing !pip install d2l==1.0.3 - currently the programs interface with d2l stored on Google Drive. These first lines can be commented out and replaced with the d2l pip install.\n","\n","\n","#P1. Concise Implementation of Softmax Regression (Section 4.5)\n","\n","\n","Experiment with the hyperparameters of the code in this section. In particular:\n","\n","**A. Plot how the validation loss changes as you change the learning rate.**\n","\n","The smaller the learning rate the larger the loss is initially. As the model has to take smaller steps as it iterates throughout training. Problem 4 will evaluate how increasing the number of epochs may allow these learning rates to ultimately converge.  \n","\n","**B. Do the validation and training loss change as you change the minibatch size? How large or small do you need to go before you see an effect?**\n","The validation and training loss most definetely change. Both losses start higher with the larger batch size. However they appear to approach convergence as the epochs increase. Especially for the validation loss the larger batch size promotes a more steady descent and less fluctuations in the validation loss size. In the small minibatches there are more variability in the graident estimates which might promote the noiser estimates seen. The power of two difference from the standard 256 seems to readily showcase differences in loss performance.\n","\n","This question will be further explored in problem 4 which utilizes an increase number of epocshs to analyze the effects\n","\n","\n","# P2. Softmax Regression Implementation from Scratch (Section 4.4)\n","In this section, we directly implemented the softmax function based on the mathematical\n","definition of the softmax operation. As discussed in Section 4.1 this can cause numerical instabilities.\n","\n","**A.Test whether softmax still works correctly if an input has a value of 100.**\n","By the above results, it is clear that an input value extremely high, like 100, causes serious exploding issues when taking the exponential of the input value because the sum of the softmax output tensor goes to infinity.\n","\n","**B. Test whether softmax still works correctly if the largest of all inputs is smaller than -100?**\n","Similarly, it is clear that taking the exponential of extremely small numbers causes the issue of underflow.\n","\n","**C. Implement a fix by looking at the value relative to the largest entry in the argument.**\n","We can see that by subtracting the largest value of the input matrix from all elements, the softmax function works properly and their output elements sum to 1.\n","\n","#P3. Softmax Regression Implementation from Scratch (Section 4.4)\n","In this problem, a new cross entropy loss function was tested and it was observed that it took much longer to run. The new cross entropy loss function requires elementwise matrix multiplication, which is much more computationally complex and time consuming. One thing to be careful of is that the logarithm function is undefined for zero or negative values, which can cause issues. To prevent this, we can clip predicted probabilities with a small positive value or add a small constant to them before taking the logarithm.\n","\n","#P4. Softmax Regression Implementation from Scratch (Section 4.4)\n","In the coincise implementation of this section:\n","\n","**A. Increase the number of epochs for training. Why might the validation accuracy decrease after a while? How could we fix this?**\n","\n","The results from the previous code utilizing 30 epochs demonstrates how the validation_acc decreases over the number of epochs. A number of reasons could contribute to this effect including overfitting and learning rate decay, where the learnign rate is too high and overshooting the optimal strategy.\n","\n","Some strategies to mitigate this effect are through utilizing dropout to regularize the model, employing early stopping,and modifying the learning rate as the program progress could all help mitigate this effect.\n","\n","\n","**B. What happens as you increase the learning rate? Compare the loss curves for several learning rates. Which one works better? When?**\n"],"metadata":{"id":"hY8BlnaeRcyF"}}]}